---
title: SyncFlow and Livekit
description: Livekit is a platform for building live video and audio applications. It provides a set of APIs and SDKs to build applications that can be used for video conferencing, live streaming, and more.
---

## MMLA Collection and Video Conferencing
 MMLA collection and analysis requirements are similar to those of video/audio conferencing applications, with an added level of complexity due to the need to support different hierarchies for group and individual audio, peripheral IoT devices, and other kinds of data discussed above. The one difference is that a video conferencing application has true peer-to-peer delivery from the get-go, whether using pure P2P protocols or an [SFU](https://trueconf.com/blog/wiki/sfu), while data collection in an MMLA setup requires centralized collection and processing with selective peer-to-peer delivery. However, this is beyond the point of analogy between video conferencing and MMLA collection.

**First**, similar to video conferencing applications, both contexts involve multiple participants (humans as well as bots).

**Second**, both require real-time data processing and delivery.

**Third**, depending on the application requirements, the software facilitating both should be scalable and delivered from the cloud.

**Fourth**, and probably the most similar aspect, is that both fundamentally include streaming audio, video, and data from multiple sources and processing them in real-time.

Similar to a timed video call in conferencing, a classroom study in MMLA has a start and end time, and the data collected during this period is saved (similar to recording online meetings), and can be processed both in real time and post-hoc. On the other side, synchronization and data quality are likely more important in MMLA collection than in video conferencing, as the data collected in MMLA should be mapped to a unified timeline and be of high quality to be used for analysis. This was probably the biggest selling point for LiveKit, as it supported all these features out of the box, and we could leverage it to fit our use case.

## LiveKit
[LiveKit](https://livekit.io) is an open-source project that enables the creation of scalable, real-time video and audio applications. It essentially allows developers to build an open-source tool similar to Zoom or Google Meet. At its core, the LiveKit server functions as a Selective Forwarding Unit ([SFU](https://trueconf.com/blog/wiki/sfu)) media server. This setup allows participants to join a LiveKit "Room" and send or receive video/audio/data to and from other peers in the room. It supports both streaming and video conferencing.

<div className="text-center p-2">
    <img src="/docs/livekit.png" alt="Livekit" className="w-1/2 mx-auto" />
    <p className="italic dark:text-white mt-2">
        A visual representation of LiveKit's SFU architecture
    </p>
</div>

One of the standout features of the LiveKit ecosystem is that it is completely open-source, and there is comprehensive documentation available on how to deploy it. LiveKit also offers a wide range of SDKs and APIs, allowing for the development of applications in languages and frameworks such as JavaScript, Python, Rust, Unity, and more. Essentially, it provides a one-stop solution for building video and audio applications. With the diversity of systems used in MMLA research, having a lot of out-of-box support is ideal.

In our specific context, the most significant advantage was LiveKit's out-of-the-box support for video, audio, and screen, as well as data streaming from the browser, which had been a major bottleneck in our ChimeraPy project. Additionally, LiveKit's Egress feature allows for saving the streams to cloud storage, with synchronization achieved via seconds from the Unix epoch as timestamps for audio and video data. Furthermore, the Agents framework in LiveKit offers a streamlined solution for processing and providing feedback on participants' streams.

## Livekit and SyncFlow Integration
